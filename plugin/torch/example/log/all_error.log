root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_broadcast/log
bash: test_broadcast/log: No such file or directory
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_broadcast.log
W0708 11:25:02.541000 3942299 site-packages/torch/distributed/run.py:793]
W0708 11:25:02.541000 3942299 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:25:02.541000 3942299 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:25:02.541000 3942299 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W708 11:25:12.465706464 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_reduce.log
W0708 11:26:38.092000 3944681 site-packages/torch/distributed/run.py:793]
W0708 11:26:38.092000 3944681 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:26:38.092000 3944681 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:26:38.092000 3944681 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W708 11:26:47.472679371 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi
example.py          fusion_result.json  run.sh              run_hetero.sh       test_broadcast.log  test_reduce.log
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_allreduce.log
W0708 11:29:28.068000 3947049 site-packages/torch/distributed/run.py:793]
W0708 11:29:28.068000 3947049 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:29:28.068000 3947049 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:29:28.068000 3947049 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W708 11:29:36.743494433 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
W0708 11:29:47.238000 3947049 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3947117 closing signal SIGTERM
W0708 11:29:47.238000 3947049 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3947118 closing signal SIGTERM
W0708 11:29:47.239000 3947049 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3947120 closing signal SIGTERM
W0708 11:29:47.239000 3947049 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3947121 closing signal SIGTERM
W0708 11:29:47.239000 3947049 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3947122 closing signal SIGTERM
W0708 11:29:47.240000 3947049 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3947123 closing signal SIGTERM
W0708 11:29:47.240000 3947049 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3947124 closing signal SIGTERM
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0708 11:29:48.374000 3947049 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -11) local_rank: 2 (pid: 3947119) of binary: /usr/local/python3.10.17/bin/python3.10
Traceback (most recent call last):
  File "/usr/local/python3.10.17/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
=========================================================
example.py FAILED
---------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
---------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-08_11:29:47
  host      : p-perf-huawei-06
  rank      : 2 (local_rank: 2)
  exitcode  : -11 (pid: 3947119)
  error_file: <N/A>
  traceback : Signal 11 (SIGSEGV) received by PID 3947119
=========================================================
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_sendrecv.log
W0708 11:30:27.705000 3949525 site-packages/torch/distributed/run.py:793]
W0708 11:30:27.705000 3949525 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:30:27.705000 3949525 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:30:27.705000 3949525 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W708 11:30:35.847152373 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
[rank0]: Traceback (most recent call last):
[rank0]:   File "/work/FlagCX/plugin/torch/example/example.py", line 341, in <module>
[rank0]:     dict_op_to_test.get(args.op, test_all)()
[rank0]:   File "/work/FlagCX/plugin/torch/example/example.py", line 316, in test_all
[rank0]:     test_sendrecv()
[rank0]:   File "/work/FlagCX/plugin/torch/example/example.py", line 133, in test_sendrecv
[rank0]:     reqs = dist.batch_isend_irecv(op_list)
[rank0]:   File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py", line 44, in _batch_isend_irecv
[rank0]:     return [_group.batch_isend_irecv(op_type, tensors, remote_rank_list)]
[rank0]: AttributeError: 'torch._C._distributed_c10d.Backend' object has no attribute 'batch_isend_irecv'
[rank4]: Traceback (most recent call last):
[rank4]:   File "/work/FlagCX/plugin/torch/example/example.py", line 341, in <module>
[rank4]:     dict_op_to_test.get(args.op, test_all)()
[rank4]:   File "/work/FlagCX/plugin/torch/example/example.py", line 316, in test_all
[rank4]:     test_sendrecv()
[rank4]:   File "/work/FlagCX/plugin/torch/example/example.py", line 133, in test_sendrecv
[rank4]:     reqs = dist.batch_isend_irecv(op_list)
[rank4]:   File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py", line 44, in _batch_isend_irecv
[rank4]:     return [_group.batch_isend_irecv(op_type, tensors, remote_rank_list)]
[rank4]: AttributeError: 'torch._C._distributed_c10d.Backend' object has no attribute 'batch_isend_irecv'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/work/FlagCX/plugin/torch/example/example.py", line 341, in <module>
[rank2]:     dict_op_to_test.get(args.op, test_all)()
[rank2]:   File "/work/FlagCX/plugin/torch/example/example.py", line 316, in test_all
[rank2]:     test_sendrecv()
[rank2]:   File "/work/FlagCX/plugin/torch/example/example.py", line 133, in test_sendrecv
[rank2]:     reqs = dist.batch_isend_irecv(op_list)
[rank2]:   File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py", line 44, in _batch_isend_irecv
[rank2]:     return [_group.batch_isend_irecv(op_type, tensors, remote_rank_list)]
[rank2]: AttributeError: 'torch._C._distributed_c10d.Backend' object has no attribute 'batch_isend_irecv'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/work/FlagCX/plugin/torch/example/example.py", line 341, in <module>
[rank6]:     dict_op_to_test.get(args.op, test_all)()
[rank6]:   File "/work/FlagCX/plugin/torch/example/example.py", line 316, in test_all
[rank6]:     test_sendrecv()
[rank6]:   File "/work/FlagCX/plugin/torch/example/example.py", line 133, in test_sendrecv
[rank6]:     reqs = dist.batch_isend_irecv(op_list)
[rank6]:   File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py", line 44, in _batch_isend_irecv
[rank6]:     return [_group.batch_isend_irecv(op_type, tensors, remote_rank_list)]
[rank6]: AttributeError: 'torch._C._distributed_c10d.Backend' object has no attribute 'batch_isend_irecv'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/work/FlagCX/plugin/torch/example/example.py", line 341, in <module>
[rank7]:     dict_op_to_test.get(args.op, test_all)()
[rank7]:   File "/work/FlagCX/plugin/torch/example/example.py", line 316, in test_all
[rank7]:     test_sendrecv()
[rank7]:   File "/work/FlagCX/plugin/torch/example/example.py", line 133, in test_sendrecv
[rank7]:     reqs = dist.batch_isend_irecv(op_list)
[rank7]:   File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py", line 44, in _batch_isend_irecv
[rank7]:     return [_group.batch_isend_irecv(op_type, tensors, remote_rank_list)]
[rank7]: AttributeError: 'torch._C._distributed_c10d.Backend' object has no attribute 'batch_isend_irecv'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/work/FlagCX/plugin/torch/example/example.py", line 341, in <module>
[rank1]:     dict_op_to_test.get(args.op, test_all)()
[rank1]:   File "/work/FlagCX/plugin/torch/example/example.py", line 316, in test_all
[rank1]:     test_sendrecv()
[rank1]:   File "/work/FlagCX/plugin/torch/example/example.py", line 133, in test_sendrecv
[rank1]:     reqs = dist.batch_isend_irecv(op_list)
[rank1]:   File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py", line 44, in _batch_isend_irecv
[rank1]:     return [_group.batch_isend_irecv(op_type, tensors, remote_rank_list)]
[rank1]: AttributeError: 'torch._C._distributed_c10d.Backend' object has no attribute 'batch_isend_irecv'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/work/FlagCX/plugin/torch/example/example.py", line 341, in <module>
[rank3]:     dict_op_to_test.get(args.op, test_all)()
[rank3]:   File "/work/FlagCX/plugin/torch/example/example.py", line 316, in test_all
[rank3]:     test_sendrecv()
[rank3]:   File "/work/FlagCX/plugin/torch/example/example.py", line 133, in test_sendrecv
[rank3]:     reqs = dist.batch_isend_irecv(op_list)
[rank3]:   File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py", line 44, in _batch_isend_irecv
[rank3]:     return [_group.batch_isend_irecv(op_type, tensors, remote_rank_list)]
[rank3]: AttributeError: 'torch._C._distributed_c10d.Backend' object has no attribute 'batch_isend_irecv'
[rank5]: Traceback (most recent call last):
[rank5]:   File "/work/FlagCX/plugin/torch/example/example.py", line 341, in <module>
[rank5]:     dict_op_to_test.get(args.op, test_all)()
[rank5]:   File "/work/FlagCX/plugin/torch/example/example.py", line 316, in test_all
[rank5]:     test_sendrecv()
[rank5]:   File "/work/FlagCX/plugin/torch/example/example.py", line 133, in test_sendrecv
[rank5]:     reqs = dist.batch_isend_irecv(op_list)
[rank5]:   File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py", line 44, in _batch_isend_irecv
[rank5]:     return [_group.batch_isend_irecv(op_type, tensors, remote_rank_list)]
[rank5]: AttributeError: 'torch._C._distributed_c10d.Backend' object has no attribute 'batch_isend_irecv'
W0708 11:30:43.060000 3949525 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3949593 closing signal SIGTERM
W0708 11:30:43.061000 3949525 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3949594 closing signal SIGTERM
W0708 11:30:43.061000 3949525 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3949595 closing signal SIGTERM
W0708 11:30:43.062000 3949525 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3949596 closing signal SIGTERM
W0708 11:30:43.063000 3949525 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3949599 closing signal SIGTERM
W0708 11:30:43.064000 3949525 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3949600 closing signal SIGTERM
E0708 11:30:43.240000 3949525 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 4 (pid: 3949597) of binary: /usr/local/python3.10.17/bin/python3.10
Traceback (most recent call last):
  File "/usr/local/python3.10.17/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-07-08_11:30:43
  host      : p-perf-huawei-06
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 3949598)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-08_11:30:43
  host      : p-perf-huawei-06
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 3949597)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_allgather.log
W0708 11:31:50.051000 3951727 site-packages/torch/distributed/run.py:793]
W0708 11:31:50.051000 3951727 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:31:50.051000 3951727 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:31:50.051000 3951727 site-packages/torch/distributed/run.py:793] *****************************************
/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
[rank0]:[W708 11:31:59.823650551 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/c10d_logger.py:83: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
W0708 11:32:08.718000 3951727 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3951795 closing signal SIGTERM
W0708 11:32:08.719000 3951727 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3951796 closing signal SIGTERM
W0708 11:32:08.719000 3951727 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3951797 closing signal SIGTERM
W0708 11:32:08.719000 3951727 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3951798 closing signal SIGTERM
W0708 11:32:08.720000 3951727 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3951799 closing signal SIGTERM
W0708 11:32:08.720000 3951727 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3951800 closing signal SIGTERM
W0708 11:32:08.721000 3951727 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3951802 closing signal SIGTERM
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0708 11:32:09.740000 3951727 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -11) local_rank: 6 (pid: 3951801) of binary: /usr/local/python3.10.17/bin/python3.10
Traceback (most recent call last):
  File "/usr/local/python3.10.17/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
=========================================================
example.py FAILED
---------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
---------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-08_11:32:08
  host      : p-perf-huawei-06
  rank      : 6 (local_rank: 6)
  exitcode  : -11 (pid: 3951801)
  error_file: <N/A>
  traceback : Signal 11 (SIGSEGV) received by PID 3951801
=========================================================
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# /usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '

root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_reducescatter.log
W0708 11:32:47.330000 3954215 site-packages/torch/distributed/run.py:793]
W0708 11:32:47.330000 3954215 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:32:47.330000 3954215 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:32:47.330000 3954215 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W708 11:32:57.038045831 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
/work/FlagCX/plugin/torch/example/example.py:206: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  dist._reduce_scatter_base(z, x, op=dist.ReduceOp.MAX, group=FLAGCX_GROUP1)
/work/FlagCX/plugin/torch/example/example.py:206: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  dist._reduce_scatter_base(z, x, op=dist.ReduceOp.MAX, group=FLAGCX_GROUP1)
/work/FlagCX/plugin/torch/example/example.py:206: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  dist._reduce_scatter_base(z, x, op=dist.ReduceOp.MAX, group=FLAGCX_GROUP1)
/work/FlagCX/plugin/torch/example/example.py:206: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  dist._reduce_scatter_base(z, x, op=dist.ReduceOp.MAX, group=FLAGCX_GROUP1)
/work/FlagCX/plugin/torch/example/example.py:206: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  dist._reduce_scatter_base(z, x, op=dist.ReduceOp.MAX, group=FLAGCX_GROUP1)
/work/FlagCX/plugin/torch/example/example.py:206: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  dist._reduce_scatter_base(z, x, op=dist.ReduceOp.MAX, group=FLAGCX_GROUP1)
/work/FlagCX/plugin/torch/example/example.py:206: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  dist._reduce_scatter_base(z, x, op=dist.ReduceOp.MAX, group=FLAGCX_GROUP1)
/work/FlagCX/plugin/torch/example/example.py:206: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  dist._reduce_scatter_base(z, x, op=dist.ReduceOp.MAX, group=FLAGCX_GROUP1)
W0708 11:33:06.196000 3954215 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3954283 closing signal SIGTERM
W0708 11:33:06.197000 3954215 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3954284 closing signal SIGTERM
W0708 11:33:06.197000 3954215 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3954286 closing signal SIGTERM
W0708 11:33:06.197000 3954215 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3954287 closing signal SIGTERM
W0708 11:33:06.198000 3954215 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3954288 closing signal SIGTERM
W0708 11:33:06.198000 3954215 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3954289 closing signal SIGTERM
W0708 11:33:06.198000 3954215 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3954290 closing signal SIGTERM
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0708 11:33:07.168000 3954215 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -11) local_rank: 2 (pid: 3954285) of binary: /usr/local/python3.10.17/bin/python3.10
Traceback (most recent call last):
  File "/usr/local/python3.10.17/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/python3.10.17/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
=========================================================
example.py FAILED
---------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
---------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-08_11:33:06
  host      : p-perf-huawei-06
  rank      : 2 (local_rank: 2)
  exitcode  : -11 (pid: 3954285)
  error_file: <N/A>
  traceback : Signal 11 (SIGSEGV) received by PID 3954285
=========================================================
/usr/local/python3.10.17/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_gather.log
W0708 11:33:37.082000 3956639 site-packages/torch/distributed/run.py:793]
W0708 11:33:37.082000 3956639 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:33:37.082000 3956639 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:33:37.082000 3956639 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W708 11:33:46.871228589 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
/usr/local/python3.10.17/lib/python3.10/site-packages/torch_npu/distributed/distributed_c10d.py:117: UserWarning: HCCL doesn't support gather at the moment. Implemented with allgather instead.
  warnings.warn("HCCL doesn't support gather at the moment. Implemented with allgather instead.")
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi
example.py              kernel_meta/            run_hetero.sh           test_allreduce.log      test_gather.log         test_reducescatter.log
fusion_result.json      run.sh                  test_allgather.log      test_broadcast.log      test_reduce.log         test_sendrecv.log
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_scatter.log
W0708 11:35:03.829000 3959078 site-packages/torch/distributed/run.py:793]
W0708 11:35:03.829000 3959078 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:35:03.829000 3959078 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:35:03.829000 3959078 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W708 11:35:12.578859803 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# ll
total 136
drwxr-xr-x.  3 root root  4096 Jul  8 11:35 ./
drwxr-xr-x.  6 root root  4096 Jul  8 09:15 ../
-rw-r--r--.  1 root root 14839 Jul  8 11:34 example.py
-rw-r--r--.  1 root root     4 Jul  8 11:35 fusion_result.json
drwxr-x---. 26 root root  4096 Jul  8 11:35 kernel_meta/
-rw-r--r--.  1 root root   750 Jul  8 09:21 run.sh
-rw-r--r--.  1 root root  1102 Jul  8 06:32 run_hetero.sh
-rw-r--r--.  1 root root 17513 Jul  8 11:32 test_allgather.log
-rw-r--r--.  1 root root 12917 Jul  8 11:29 test_allreduce.log
-rw-r--r--.  1 root root  7654 Jul  8 11:25 test_broadcast.log
-rw-r--r--.  1 root root 11258 Jul  8 11:33 test_gather.log
-rw-r--r--.  1 root root  7206 Jul  8 11:26 test_reduce.log
-rw-r--r--.  1 root root 15833 Jul  8 11:33 test_reducescatter.log
-rw-r--r--.  1 root root 11350 Jul  8 11:35 test_scatter.log
-rw-r--r--.  1 root root  3837 Jul  8 11:30 test_sendrecv.log
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# vi example.py
root@p-perf-huawei-06:/work/FlagCX/plugin/torch/example# bash run.sh > test_alltoall.log
W0708 11:36:06.595000 3961437 site-packages/torch/distributed/run.py:793]
W0708 11:36:06.595000 3961437 site-packages/torch/distributed/run.py:793] *****************************************
W0708 11:36:06.595000 3961437 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performancein your application as needed.
W0708 11:36:06.595000 3961437 site-packages/torch/distributed/run.py:793] *****************************************
[rank0]:[W708 11:36:20.556975144 compiler_depend.ts:137] Warning: Warning: Device do not support double dtype now, dtype cast repalce with float. (function operator())
